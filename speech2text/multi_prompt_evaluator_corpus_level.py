#!/usr/bin/env python3
"""
QwenVL multi-prompt evaluation script - corpus-level standalone v2
Reimplemented to match the design pattern of qwenvl_whisper_fusion_2.py.

Key features:
1. No Ground Truth dependency: uniformly extract one frame per second, not by subtitle timestamps
2. Deduplication: keep only one from consecutive identical results
3. Corpus-level evaluation: compute metrics on the fully concatenated text, not per-item averages
4. Caching and processing aligned with qwenvl_whisper_fusion_2.py

Capabilities:
1. Run end-to-end generation and evaluation for each prompt
2. Compare prompt performance (corpus-level metrics)
3. Produce detailed comparison reports
4. Identify the best prompt

Usage:
    # Evaluate all prompts
    python multi_prompt_evaluator_corpus_level_v2.py

    # Evaluate specific prompts
    python multi_prompt_evaluator_corpus_level_v2.py --prompts ocr_focused subtitle_specific

    # Limit number of videos
    python multi_prompt_evaluator_corpus_level_v2.py --max-videos 3
"""

import argparse
import json
import time
import pandas as pd
import sacrebleu
import jiwer
import cv2
import base64
import requests
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass
import re


def deduplicate_consecutive_predictions(predictions: List[str]) -> List[str]:
    """Deduplicate consecutive identical predictions."""
    if not predictions:
        return []
    
    deduplicated = [predictions[0]]
    for i in range(1, len(predictions)):
        # Simple text similarity check
        if predictions[i].strip() != predictions[i-1].strip():
            deduplicated.append(predictions[i])
    
    return deduplicated


class CorpusLevelEvaluator:
    """Corpus-level evaluator aligned with qwenvl_whisper_fusion_2.py."""
    
    def __init__(self, video_dir: str, subtitle_dir: str = None):
        self.video_dir = Path(video_dir)
        self.subtitle_dir = Path(subtitle_dir) if subtitle_dir else self.video_dir
        
        # Cache directories aligned with qwenvl_whisper_fusion_2.py
        self.frames_cache_dir = Path("speech2text/frames_cache_hybrid")
        self.frames_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # QwenVL results cache directory
        self.qwenvl_cache_dir = Path("speech2text/qwenvl_cache_hybrid_multi_prompt_3090")
        self.qwenvl_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Set up logging
        self.logger = self._setup_logging()
        
        # API configuration
        self.qwenvl_api_url = "http://localhost:8000/v1/chat/completions"
        self.model_name = "Qwen/Qwen2-VL-2B-Instruct"
        self.frame_interval = 1.0  # Extract one frame per second
        
        self.logger.info(f"åˆå§‹åŒ–Corpus-levelè¯„ä¼°å™¨")
        self.logger.info(f"è§†é¢‘ç›®å½•: {self.video_dir}")
        self.logger.info(f"å­—å¹•ç›®å½•: {self.subtitle_dir}")
        self.logger.info(f"å¸§ç¼“å­˜ç›®å½•: {self.frames_cache_dir}")
        self.logger.info(f"QwenVLç¼“å­˜ç›®å½•: {self.qwenvl_cache_dir}")
        
    def _setup_logging(self) -> logging.Logger:
        """Set up logging."""
        logger = logging.getLogger("CorpusLevelEvaluator")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger

    def get_video_info(self, video_path: str) -> Tuple[float, float, int]:
        """Get basic video info."""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps if fps > 0 else 0
        
        cap.release()
        return duration, fps, frame_count

    def extract_frames_uniform(self, video_path: str, video_name: str) -> List[Tuple[int, float, str]]:
        """
        Extract frames at a fixed interval (with caching), aligned with
        qwenvl_whisper_fusion_2.py.
        
        Returns:
            List of (frame_index, timestamp, frame_image_path)
        """
        # Check for cache
        video_cache_dir = self.frames_cache_dir / video_name
        if self._check_frame_cache_exists(video_name):
            self.logger.info(f"ä»ç¼“å­˜åŠ è½½è§†é¢‘å¸§: {video_name}")
            return self._load_frames_from_cache(video_name)
        
        self.logger.info(f"æå–è§†é¢‘å¸§: {video_name}")
        video_cache_dir.mkdir(exist_ok=True)
        
        # Get video info
        duration, fps, frame_count = self.get_video_info(video_path)
        
        cap = cv2.VideoCapture(video_path)
        frames_info = []
        
        # Extract frames at fixed intervals
        current_time = 0.0
        while current_time < duration:
            frame_index = int(current_time * fps)
            if frame_index >= frame_count:
                break
                
            # Seek to the target frame
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
            ret, frame = cap.read()
            
            if ret:
                # Save frame to cache
                frame_filename = f"frame_{frame_index}_{current_time:.2f}.jpg"
                frame_path = video_cache_dir / frame_filename
                cv2.imwrite(str(frame_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                
                frames_info.append((frame_index, current_time, str(frame_path)))
                self.logger.debug(f"æå–å¸§: {frame_index} @ {current_time:.2f}s")
            
            current_time += self.frame_interval
        
        cap.release()
        self.logger.info(f"å…±æå– {len(frames_info)} å¸§åˆ°ç¼“å­˜")
        return frames_info
    
    def _check_frame_cache_exists(self, video_name: str) -> bool:
        """Check whether frame cache exists."""
        video_cache_dir = self.frames_cache_dir / video_name
        if not video_cache_dir.exists():
            return False
        
        frame_files = list(video_cache_dir.glob("frame_*.jpg"))
        return len(frame_files) > 0
    
    def _load_frames_from_cache(self, video_name: str) -> List[Tuple[int, float, str]]:
        """Load frame info from cache."""
        video_cache_dir = self.frames_cache_dir / video_name
        frame_files = sorted(video_cache_dir.glob("frame_*.jpg"))
        
        frames_info = []
        for frame_file in frame_files:
            # Parse frame info from filename: frame_{index}_{timestamp}.jpg
            name_parts = frame_file.stem.split('_')
            if len(name_parts) >= 3:
                try:
                    frame_index = int(name_parts[1])
                    timestamp = float(name_parts[2])
                    frames_info.append((frame_index, timestamp, str(frame_file)))
                except (ValueError, IndexError):
                    continue
        
        return frames_info
    
    def _check_qwenvl_cache_exists(self, video_name: str, prompt_name: str) -> bool:
        """Check whether the QwenVL cache exists."""
        cache_file = self.qwenvl_cache_dir / f"{video_name}_{prompt_name}_qwenvl_results.json"
        return cache_file.exists()
    
    def _save_qwenvl_cache(self, video_name: str, prompt_name: str, subtitles: List[Tuple[float, str]]):
        """Save QwenVL results to cache."""
        cache_file = self.qwenvl_cache_dir / f"{video_name}_{prompt_name}_qwenvl_results.json"
        cache_data = {
            "video_name": video_name,
            "prompt_name": prompt_name,
            "timestamp": datetime.now().isoformat(),
            "subtitles": subtitles,
            "frame_interval": self.frame_interval
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            self.logger.debug(f"QwenVLç»“æœå·²ç¼“å­˜: {cache_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜QwenVLç¼“å­˜å¤±è´¥: {e}")
    
    def _load_qwenvl_cache(self, video_name: str, prompt_name: str) -> List[Tuple[float, str]]:
        """Load QwenVL results from cache."""
        cache_file = self.qwenvl_cache_dir / f"{video_name}_{prompt_name}_qwenvl_results.json"
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Check whether the cache matches current configuration
            if cache_data.get("frame_interval") == self.frame_interval:
                subtitles = [(float(ts), text) for ts, text in cache_data["subtitles"]]
                self.logger.info(f"ä»ç¼“å­˜åŠ è½½QwenVLç»“æœ: {len(subtitles)} ä¸ªå­—å¹•ç‰‡æ®µ")
                return subtitles
            else:
                self.logger.warning(f"QwenVLç¼“å­˜é…ç½®ä¸åŒ¹é…ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                return []
                
        except Exception as e:
            self.logger.error(f"åŠ è½½QwenVLç¼“å­˜å¤±è´¥: {e}")
            return []

    def clear_qwenvl_cache(self, video_name: str = None, prompt_name: str = None):
        """
        Clear QwenVL cache.

        Args:
            video_name: Specific video name.
            prompt_name: Specific prompt name.
            If both are None, clear all caches.
        """
        if video_name and prompt_name:
            cache_file = self.qwenvl_cache_dir / f"{video_name}_{prompt_name}_qwenvl_results.json"
            if cache_file.exists():
                cache_file.unlink()
                self.logger.info(f"å·²æ¸…ç†QwenVLç¼“å­˜: {video_name}_{prompt_name}")
        else:
            # Clear all caches
            for cache_file in self.qwenvl_cache_dir.glob("*_qwenvl_results.json"):
                cache_file.unlink()
            self.logger.info("å·²æ¸…ç†æ‰€æœ‰QwenVLç¼“å­˜")

    def extract_subtitles_with_qwenvl(self, frame_paths: List[Tuple[int, float, str]], video_name: str, prompt_template: str, prompt_name: str) -> List[Tuple[float, str]]:
        """
        Extract subtitles with QwenVL (with caching), aligned with
        qwenvl_whisper_fusion_2.py.
        
        Args:
            frame_paths: List of (frame_index, timestamp, frame_image_path)
            video_name: Video name.
            prompt_template: Prompt template.
            prompt_name: Prompt name (for caching).
            
        Returns:
            List of (timestamp, subtitle_text)
        """
        # Check cache first
        if self._check_qwenvl_cache_exists(video_name, prompt_name):
            self.logger.info(f"å‘ç°QwenVLç¼“å­˜ï¼Œç›´æ¥åŠ è½½: {video_name}_{prompt_name}")
            cached_subtitles = self._load_qwenvl_cache(video_name, prompt_name)
            if cached_subtitles:  # Return directly if cache is valid
                return cached_subtitles
        
        # Cache missing or invalid; call QwenVL
        self.logger.info(f"ä½¿ç”¨QwenVLæå– {len(frame_paths)} å¸§å­—å¹•ï¼Œprompt: {prompt_name}")
        
        from tqdm import tqdm
        subtitles = []
        for frame_index, timestamp, frame_path in tqdm(frame_paths, desc=f"QwenVLå­—å¹•æå–", unit="å¸§"):
            try:
                # Read frame image and encode
                frame = cv2.imread(frame_path)
                if frame is None:
                    continue
                # Convert to RGB and encode as JPEG
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                success, buffer = cv2.imencode('.jpg', frame_rgb)
                if not success:
                    continue
                image_base64 = base64.b64encode(buffer).decode('utf-8')
                # Call QwenVL API
                subtitle_text = self._call_qwenvl_api(image_base64, prompt_template)
                # Save all results (including "no subtitle"); dedup happens later
                subtitles.append((timestamp, subtitle_text.strip()))
                self.logger.debug(f"å¸§ {frame_index} @ {timestamp:.2f}s: {subtitle_text}")
            except Exception as e:
                self.logger.error(f"å¤„ç†å¸§ {frame_index} æ—¶å‡ºé”™: {e}")
                continue
        # Save results to cache
        self._save_qwenvl_cache(video_name, prompt_name, subtitles)
        self.logger.info(f"QwenVLæˆåŠŸæå– {len(subtitles)} ä¸ªå­—å¹•ç‰‡æ®µ")
        return subtitles

    def _call_qwenvl_api(self, image_base64: str, prompt_template: str) -> str:
        """Call the QwenVL API, aligned with qwenvl_whisper_fusion_2.py."""
        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        },
                        {
                            "type": "text",
                            "text": prompt_template
                        }
                    ]
                }
            ],
            "max_tokens": 200,
            "temperature": 0.0
        }
        
        try:
            response = requests.post(
                self.qwenvl_api_url,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
            
        except Exception as e:
            self.logger.error(f"QwenVL APIè°ƒç”¨å¤±è´¥: {e}")
            return ""

    def deduplicate_adjacent_subtitles(self, subtitles: List[Tuple[float, str]]) -> List[str]:
        """
        Deduplicate adjacent subtitles and extract text, aligned with
        qwenvl_whisper_fusion_2.py.
        
        Args:
            subtitles: [(timestamp, subtitle_text), ...]
            
        Returns:
            List of deduplicated texts.
        """
        if not subtitles:
            return []
        
        # Key fix: sort by timestamp to avoid cache ordering issues
        subtitles.sort(key=lambda x: x[0])
        self.logger.info(f"æŒ‰æ—¶é—´æˆ³æ’åºäº† {len(subtitles)} ä¸ªå­—å¹•ç‰‡æ®µ")
        
        merged_texts = []
        prev_text = ""
        
        for timestamp, text in subtitles:
            # Filter out "no subtitle" and empty text
            # if text and text.strip() not in ['æ— å­—å¹•', '[ç©º]', '']:
            #     # Simple dedup: keep if different from previous text
            #     if text != prev_text:
            #         merged_texts.append(text)
            #         prev_text = text
            #         self.logger.debug(f"Add new subtitle @ {timestamp:.2f}s: {text}")
            #     else:
            #         self.logger.debug(f"Skip duplicate subtitle @ {timestamp:.2f}s: {text}")

            if text != prev_text:
                merged_texts.append(text)
                prev_text = text
                self.logger.debug(f"Add new subtitle @ {timestamp:.2f}s: {text}")
            else:
                self.logger.debug(f"Skip duplicate subtitle @ {timestamp:.2f}s: {text}")  
                      
        self.logger.info(f"å­—å¹•å»é‡: {len(subtitles)} -> {len(merged_texts)} ä¸ªç‹¬ç‰¹ç‰‡æ®µ")
        
        # Terminal preview: show the first 10 subtitles after sorting
        if merged_texts:
            preview_count = min(10, len(merged_texts))
            preview_text = " | ".join(merged_texts[:preview_count])
            self.logger.info(f"å»é‡åå­—å¹•é¢„è§ˆï¼ˆå‰{preview_count}æ¡ï¼‰: {preview_text}")
        
        return merged_texts

    def get_complete_subtitle_text(self, video_name: str) -> str:
        """Get full text directly from subtitle files (for corpus-level eval)."""
        base_name = video_name.replace('.mp4', '')
        
        # Try SRT subtitle file formats
        possible_files = [
            self.subtitle_dir / f"{base_name}æ–°.srt",
            self.subtitle_dir / f"{base_name}.srt"
        ]
        
        for subtitle_path in possible_files:
            if subtitle_path.exists():
                return self._parse_srt_to_text(subtitle_path)
        
        self.logger.warning(f"æœªæ‰¾åˆ°è§†é¢‘ {video_name} çš„å­—å¹•æ–‡ä»¶")
        return ""
    
    def _parse_srt_to_text(self, srt_path: Path) -> str:
        """Parse an SRT file into plain text."""
        try:
            with open(srt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.strip().split('\n')
            text_lines = []
            
            for line in lines:
                line = line.strip()
                # Skip index lines
                if line.isdigit():
                    continue
                # Skip timestamp lines
                if '-->' in line:
                    continue
                # Skip empty lines
                if not line:
                    continue
                
                # Remove speaker identifiers
                line = re.sub(r'^\d+\s+', '', line)
                text_lines.append(line)
            
            return " ".join(text_lines)
            
        except Exception as e:
            self.logger.error(f"è§£æSRTæ–‡ä»¶å¤±è´¥ {srt_path}: {e}")
            return ""

    def calculate_corpus_level_metrics_direct(self, prediction_text: str, reference_text: str, system_name: str) -> Dict:
        """Compute corpus-level metrics on concatenated text."""
        self.logger.info(f"è®¡ç®— {system_name} çš„ corpus-level æŒ‡æ ‡...")
        
        if not prediction_text or not reference_text:
            return {
                'bleu_score': 0.0,
                'chrf_score': 0.0,
                'cer': 1.0,
                'character_accuracy': 0.0,
                'composite_score': 0.0,
                'total_pairs': 0
            }
        
        # Log corpus-level documents
        self.logger.info(f"Corpus-level æ–‡æ¡£:")
        self.logger.info(f"   é¢„æµ‹æ–‡æ¡£æ€»é•¿åº¦: {len(prediction_text)} å­—ç¬¦")
        self.logger.info(f"   å‚è€ƒæ–‡æ¡£æ€»é•¿åº¦: {len(reference_text)} å­—ç¬¦")
        
        # Log the first 200 characters used for evaluation
        self.logger.info(f"   é¢„æµ‹æ–‡æœ¬å‰200å­—ç¬¦: {prediction_text[:200]}...")
        self.logger.info(f"   å‚è€ƒæ–‡æœ¬å‰200å­—ç¬¦: {reference_text[:200]}...")
        
        # 1. BLEU (corpus-level) - treat the text as a single sentence
        bleu = sacrebleu.corpus_bleu([prediction_text], [[reference_text]], tokenize='zh')
        
        # 2. chrF++ (corpus-level)
        chrf = sacrebleu.corpus_chrf([prediction_text], [[reference_text]], word_order=2)
        
        # 3. CER (corpus-level)
        corpus_cer = jiwer.cer(reference_text, prediction_text)
        
        # 4. Character accuracy (corpus-level)
        if reference_text:
            output = jiwer.process_characters(reference_text, prediction_text)
            correct_chars = output.hits
            total_chars_in_ref = output.hits + output.substitutions + output.deletions
            corpus_char_accuracy = correct_chars / total_chars_in_ref if total_chars_in_ref > 0 else 0.0
        else:
            corpus_char_accuracy = 1.0 if not prediction_text else 0.0
        
        # 5. Composite score
        bleu_normalized = bleu.score / 100.0
        chrf_normalized = chrf.score / 100.0
        cer_accuracy = max(0.0, 1.0 - corpus_cer)
        composite_score = (bleu_normalized + chrf_normalized + cer_accuracy + corpus_char_accuracy) / 4.0
        
        metrics = {
            'bleu_score': bleu.score,
            'chrf_score': chrf.score,
            'cer': corpus_cer,
            'character_accuracy': corpus_char_accuracy,
            'composite_score': composite_score,
            'bleu_normalized': bleu_normalized,
            'chrf_normalized': chrf_normalized,
            'cer_accuracy': cer_accuracy,
            'total_pairs': 1,  # Whole corpus as one comparison pair
            'total_pred_chars': len(prediction_text),
            'total_ref_chars': len(reference_text)
        }
        
        self.logger.info(f"{system_name} corpus-level æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        self.logger.info(f"   - BLEU: {metrics['bleu_score']:.4f}")
        self.logger.info(f"   - chrF++: {metrics['chrf_score']:.4f}")
        self.logger.info(f"   - CER: {metrics['cer']:.4f}")
        self.logger.info(f"   - å­—ç¬¦å‡†ç¡®ç‡: {metrics['character_accuracy']:.4f}")
        self.logger.info(f"   - ç»¼åˆæŒ‡æ ‡: {composite_score:.4f}")
        
        return metrics

    def evaluate_single_prompt(self, prompt_name: str, prompt_template: str, max_videos: int = 0) -> Dict[str, Any]:
        """Evaluate a single prompt."""
        self.logger.info(f"å¼€å§‹è¯„ä¼° Prompt: {prompt_name}")
        self.logger.info("=" * 60)
        
        # Get video list
        video_files = list(self.video_dir.glob('*.mp4'))
        if max_videos > 0:
            video_files = video_files[:max_videos]
        
        self.logger.info(f"å°†å¤„ç† {len(video_files)} ä¸ªè§†é¢‘")
        
        all_predictions = []
        video_details = []
        
        start_time = time.time()
        
        for video_file in video_files:
            self.logger.info(f"å¤„ç†è§†é¢‘: {video_file.name}")
            
            video_base_name = video_file.stem
            
            # Extract frames
            frame_paths = self.extract_frames_uniform(str(video_file), video_base_name)
            self.logger.info(f"æå–äº† {len(frame_paths)} å¸§")
            
            # Extract subtitles with QwenVL (cached)
            subtitles = self.extract_subtitles_with_qwenvl(
                frame_paths, video_base_name, prompt_template, prompt_name
            )
            
            # Apply deduplication and extract text
            video_predictions = self.deduplicate_adjacent_subtitles(subtitles)
            self.logger.info(f"å»é‡å¤„ç†: {len(subtitles)} -> {len(video_predictions)} æ¡")
            
            all_predictions.extend(video_predictions)
            
            # Collect per-video details
            video_details.append({
                'video_name': video_file.name,
                'original_prediction_count': len(subtitles),
                'deduplicated_prediction_count': len(video_predictions),
                'predictions': " | ".join(video_predictions)
            })
        
        processing_time = time.time() - start_time
        self.logger.info(f"{prompt_name} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
        
        # Concatenate predictions across all videos
        corpus_prediction_text = " ".join(all_predictions)
        
        # Fetch and concatenate references for all videos
        self.logger.info("æ­£åœ¨å‡†å¤‡ Corpus-level å‚è€ƒæ–‡æœ¬...")
        all_references = []
        for video_file in video_files:
            ref_text = self.get_complete_subtitle_text(video_file.name)
            if ref_text:
                all_references.append(ref_text)
        corpus_reference_text = " ".join(all_references)
        self.logger.info("å‚è€ƒæ–‡æœ¬å‡†å¤‡å®Œæˆ")
        
        # Compute corpus-level metrics
        metrics = self.calculate_corpus_level_metrics_direct(
            corpus_prediction_text, 
            corpus_reference_text,
            prompt_name
        )
        
        return {
            "prompt_name": prompt_name,
            "metrics": metrics,
            "total_videos": len(video_files),
            "processing_time": processing_time,
            "video_details": video_details
        }


def main():
    """Main entry point."""
    # Prompt configuration
    PROMPT_CANDIDATES = {
        "simple_direct": "è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡å­—ï¼š",
        "few_shot_simple": "ä»å›¾ç‰‡ä¸­æå–å­—å¹•ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå›¾ç‰‡å­—å¹•æ˜¯â€œä½ å¥½ä¸–ç•Œâ€ï¼Œä½ åº”è¯¥åªè¾“å‡ºâ€œä½ å¥½ä¸–ç•Œâ€ã€‚ç°åœ¨ï¼Œè¯·å¤„ç†è¿™å¼ å›¾ç‰‡ã€‚",
        "ocr_focused": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„OCRæ–‡å­—è¯†åˆ«å¼•æ“ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç²¾ç¡®åœ°è½¬å½•è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰å­—å¹•æ–‡æœ¬ï¼Œç¡®ä¿100%çš„å‡†ç¡®æ€§ã€‚è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡å­—ï¼Œåªè¿”å›è¯†åˆ«åˆ°çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œè¿”å›'æ— å­—å¹•'ã€‚",
        "subtitle_specific": "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡å­—ã€‚è¦æ±‚ï¼š1)åªè¿”å›å­—å¹•å†…å®¹ï¼Œä¸è¦å…¶ä»–æè¿° 2)ä¿æŒåŸæ–‡æ ¼å¼ 3)å¦‚æœæ²¡æœ‰å­—å¹•è¿”å›'æ— å­—å¹•'",
        "context_aware_new": "è¿™æ˜¯ä¸€ä¸ªè§†é¢‘æˆªå›¾ï¼Œè¯·è¯†åˆ«å…¶ä¸­çš„ä¸­æ–‡å­—å¹•å†…å®¹ã€‚åªè¿”å›å­—å¹•æ–‡å­—ï¼Œå¿½ç•¥å…¶ä»–å›¾åƒå…ƒç´ ã€‚å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œè¯·è¾“å‡º[ç©º]ã€‚",
        "chain_of_thought": "è¯·åˆ†æ­¥æ‰§è¡Œä»»åŠ¡ï¼š1. å®šä½å›¾ç‰‡ä¸­å­—å¹•çš„ä½ç½®ã€‚2. å¿½ç•¥èƒŒæ™¯ä¸­çš„å¹²æ‰°å…ƒç´ ã€‚3. ä»”ç»†è¯†åˆ«å¹¶è½¬å½•ä½ æ‰¾åˆ°çš„å­—å¹•ã€‚æœ€åï¼Œåªè¾“å‡ºæœ€ç»ˆè½¬å½•çš„å­—å¹•æ–‡æœ¬ã€‚",
        "robustness_instruction": "è¿™å¼ å›¾ç‰‡çš„èƒŒæ™¯å¯èƒ½å¾ˆå¤æ‚ï¼Œå­—å¹•å¯èƒ½æœ‰å¤šç§é¢œè‰²æˆ–è¢«è½»å¾®é®æŒ¡ã€‚è¯·å°½æœ€å¤§åŠªåŠ›ï¼Œç²¾ç¡®åœ°æå–å‡ºæ‰€æœ‰å¯è§çš„å­—å¹•æ–‡æœ¬ã€‚"
    }
    
    parser = argparse.ArgumentParser(
        description='QwenVL å¤š Prompt å¾ªç¯è¯„ä¼°è„šæœ¬ - Corpus-level ç‹¬ç«‹ç‰ˆæœ¬ v2',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
é‡è¦ç‰¹æ€§:
  - å‡åŒ€æå–: æ¯ç§’æå–ä¸€å¸§ï¼Œä¸ä¾èµ– ground truth
  - å»é‡å¤„ç†: ç›¸é‚»ç›¸åŒé¢„æµ‹ç»“æœè‡ªåŠ¨å»é‡  
  - Corpus-level è¯„ä¼°: æ•´ä½“æ‹¼æ¥è®¡ç®—æŒ‡æ ‡ï¼Œè€Œéé€é¡¹å¹³å‡
  - å®Œå…¨å‘ qwenvl_whisper_fusion_2.py çœ‹é½çš„ç¼“å­˜å’Œå¤„ç†æœºåˆ¶

å¯ç”¨çš„ Prompt:
{chr(10).join(f"  - {name}: {template[:60]}..." for name, template in PROMPT_CANDIDATES.items())}

ä½¿ç”¨ç¤ºä¾‹:
  # è¯„ä¼°æ‰€æœ‰ prompt
  python multi_prompt_evaluator_corpus_level_v2.py
  
  # è¯„ä¼°æŒ‡å®šçš„ prompt
  python multi_prompt_evaluator_corpus_level_v2.py --prompts simple_direct ocr_focused
  
  # é™åˆ¶æµ‹è¯•è§†é¢‘æ•°é‡ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
  python multi_prompt_evaluator_corpus_level_v2.py --max-videos 2
        """
    )
    
    parser.add_argument('--prompts', nargs='+', 
                       choices=list(PROMPT_CANDIDATES.keys()),
                       help='æŒ‡å®šè¦è¯„ä¼°çš„promptåç§°')
    
    parser.add_argument('--max-videos', type=int, default=0,
                       help='é™åˆ¶æœ€å¤§è§†é¢‘æ•°é‡ (0=ä¸é™åˆ¶)')
    
    parser.add_argument('--video-dir', default="data/é—ªå©šå¹¸è¿è‰çš„å‘½ä¸­æ³¨å®š",
                       help='è§†é¢‘ç›®å½•è·¯å¾„')
    
    parser.add_argument('--subtitle-dir', default="data/é—ªå©šå¹¸è¿è‰çš„å‘½ä¸­æ³¨å®š/å¸¦è§’è‰²æ ‡æ³¨çš„å­—å¹•", 
                       help='å­—å¹•ç›®å½•è·¯å¾„')
    
    parser.add_argument('--clear-cache', action='store_true',
                       help='æ¸…ç†æ‰€æœ‰QwenVLç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°å¤„ç†')
    
    args = parser.parse_args()
    
    print("ğŸš€ QwenVL å¤š Prompt å¾ªç¯è¯„ä¼°è„šæœ¬ - Corpus-level ç‹¬ç«‹ç‰ˆæœ¬ v2")
    print("ğŸ”¬ æ¯ç§’å‡åŒ€æå– + å»é‡ + æ•´ä½“æ‹¼æ¥è¯„ä¼°")
    print("âš ï¸  æ³¨æ„: ä¸ä¾èµ– ground truthï¼Œåªåœ¨æœ€ç»ˆè¯„ä¼°æ—¶ä½¿ç”¨å‚è€ƒæ–‡æœ¬")
    
    # Determine prompts to evaluate
    if args.prompts:
        prompts_to_evaluate = {name: PROMPT_CANDIDATES[name] for name in args.prompts if name in PROMPT_CANDIDATES}
    else:
        prompts_to_evaluate = PROMPT_CANDIDATES
    
    print(f"ğŸ“‹ å°†è¯„ä¼° {len(prompts_to_evaluate)} ä¸ª Prompt")
    
    # Create evaluator
    evaluator = CorpusLevelEvaluator(args.video_dir, args.subtitle_dir)
    
    # Clear cache if requested
    if args.clear_cache:
        evaluator.clear_qwenvl_cache()
        print("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰QwenVLç¼“å­˜")
    
    # Create output directory
    output_dir = Path("speech2text/corpus_level_evaluation_results")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Run evaluation
    all_results = []
    total_start_time = time.time()
    
    for i, (prompt_name, prompt_template) in enumerate(prompts_to_evaluate.items(), 1):
        print(f"\n{'='*20} [{i}/{len(prompts_to_evaluate)}] {'='*20}")
        
        try:
            result = evaluator.evaluate_single_prompt(prompt_name, prompt_template, args.max_videos)
            all_results.append(result)
            
            print(f"âœ… {prompt_name} è¯„ä¼°å®Œæˆ (é¢„æµ‹ç»“æœå·²ç¼“å­˜åˆ° {evaluator.qwenvl_cache_dir})")
            
        except Exception as e:
            print(f"âŒ {prompt_name} è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    total_time = time.time() - total_start_time
    
    # Generate final ranking
    if all_results:
        print(f"\nğŸ† æœ€ç»ˆ Corpus-level æ’åº (æŒ‰ç»¼åˆæŒ‡æ ‡):")
        print("-" * 80)
        
        sorted_results = sorted(all_results, 
                              key=lambda x: x['metrics']['composite_score'], 
                              reverse=True)
        
        for rank, result in enumerate(sorted_results, 1):
            metrics = result['metrics']
            print(f"#{rank:2d}. {result['prompt_name']:<25} "
                  f"ç»¼åˆ: {metrics['composite_score']:.4f} "
                  f"BLEU: {metrics['bleu_score']:.4f} "
                  f"chrF++: {metrics['chrf_score']:.4f} "
                  f"CER: {metrics['cer']:.4f}")
        
        # Save summary
        summary_data = []
        for rank, result in enumerate(sorted_results, 1):
            metrics = result['metrics']
            summary_data.append({
                'rank': rank,
                'prompt_name': result['prompt_name'],
                'composite_score': metrics['composite_score'],
                'bleu_score': metrics['bleu_score'],
                'chrf_score': metrics['chrf_score'],
                'cer': metrics['cer'],
                'character_accuracy': metrics['character_accuracy'],
                'total_videos': result['total_videos'],
                'processing_time': result['processing_time'],
                'methodology': 'corpus_level_uniform_extraction_with_deduplication'
            })
        
        summary_csv = output_dir / f"corpus_level_ranking_{timestamp}.csv"
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_csv(summary_csv, index=False, encoding='utf-8')
        
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"  - corpus_level_ranking_{timestamp}.csv: æœ€ç»ˆæ’åº")
        print(f"  - QwenVLé¢„æµ‹ç»“æœç¼“å­˜ç›®å½•: {evaluator.qwenvl_cache_dir}")
        print(f"  - è§†é¢‘å¸§ç¼“å­˜ç›®å½•: {evaluator.frames_cache_dir}")
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")


if __name__ == "__main__":
    main()
