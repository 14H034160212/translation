#!/usr/bin/env python3
"""
Unified Evaluation Framework
Provides consistent evaluation interface and metrics calculation for QwenVL and Whisper systems.
"""

import re
import json
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import jiwer
import sacrebleu


class EvaluationMetrics:
    """Unified metrics calculator ensuring all systems use consistent methods."""
    
    @staticmethod
    def clean_text(text: str) -> str:
        return text
        # """Standardized text cleaning method."""
        # if not text:
        #     return ""

        # # Remove subtitle prefix
        # text = re.sub(r'^å­—å¹•[ï¼š:]\s*', '', text)
        
        # # Handle empty content
        # if text.strip() in ['æ— ', 'æ— å­—å¹•', 'å­—å¹•: æ— ', 'å­—å¹•ï¼šæ— ']:
        #     return ""

        # # Remove anomalous output patterns (prompt content, etc.)
        # anomaly_patterns = [
        #     "è¿™æ˜¯ä¸€ä¸ªè§†é¢‘æˆªå›¾ï¼Œè¯·è¯†åˆ«å…¶ä¸­çš„ä¸­æ–‡å­—å¹•å†…å®¹ã€‚åªè¿”å›žå­—å¹•æ–‡å­—ï¼Œå¿½ç•¥å…¶ä»–å›¾åƒå…ƒç´ ã€‚",
        #     "è¿™æ˜¯ä¸€ä¸ªè§†é¢‘æˆªå›¾ï¼Œè¯·è¯†åˆ«å…¶ä¸­çš„ä¸­æ–‡å­—å¹•å†…å®¹",
        #     "è¯·è¯†åˆ«å…¶ä¸­çš„ä¸­æ–‡å­—å¹•å†…å®¹",
        #     "åªè¿”å›žå­—å¹•æ–‡å­—ï¼Œå¿½ç•¥å…¶ä»–å›¾åƒå…ƒç´ ",
        #     "è¯†åˆ«å›¾ç‰‡ä¸­çš„å­—å¹•æ–‡å­—"
        # ]
        # text_stripped = text.strip()
        # for pattern in anomaly_patterns:
        #     if pattern in text_stripped:
        #         return ""

        # # Check for too many prompt keywords
        # prompt_keywords = ['è§†é¢‘æˆªå›¾', 'è¯†åˆ«å…¶ä¸­', 'å­—å¹•å†…å®¹', 'å›¾åƒå…ƒç´ ', 'åªè¿”å›ž', 'å¿½ç•¥å…¶ä»–']
        # keyword_count = sum(1 for keyword in prompt_keywords if keyword in text_stripped)
        # if keyword_count >= 3:
        #     return ""

        # # Normalize whitespace
        # return re.sub(r'\s+', ' ', text.strip())

    @staticmethod
    def calculate_cer(predicted: str, reference: str) -> float:
        """Calculate Character Error Rate (CER) using jiwer."""
        pred_clean = EvaluationMetrics.clean_text(predicted)
        ref_clean = EvaluationMetrics.clean_text(reference)

        if not ref_clean:
            return 0.0 if not pred_clean else 1.0
        if not pred_clean:
            return 1.0

        # Compute CER at the character level
        return jiwer.cer(ref_clean, pred_clean)

    @staticmethod
    def calculate_character_accuracy(predicted: str, reference: str) -> float:
        """Calculate character accuracy (correct chars / total reference chars)."""
        pred_clean = EvaluationMetrics.clean_text(predicted)
        ref_clean = EvaluationMetrics.clean_text(reference)
        if not ref_clean:
            return 1.0 if not pred_clean else 0.0
        
        if not pred_clean:
            return 0.0

        import jiwer
        
        ref_chars = list(ref_clean)
        pred_chars = list(pred_clean)
        
        # Compute detailed character-level metrics
        output = jiwer.process_characters(ref_clean, pred_clean)
        measures = { 'hits': output.hits, 'substitutions': output.substitutions, 'deletions': output.deletions }

        # 4. Extract core data required for calculation
        # measures['hits'] is the number of correct characters
        correct_chars = measures['hits']
        
        # Total reference characters N = hits (H) + substitutions (S) + deletions (D)
        substitutions = measures['substitutions']
        deletions = measures['deletions']
        total_chars_in_ref = correct_chars + substitutions + deletions

        # 5. Compute final accuracy
        # Re-check denominator to avoid division by zero after cleaning
        if total_chars_in_ref == 0:
            # Reference text was cleaned to empty; safeguard against divide-by-zero
            return 1.0 if not pred_clean else 0.0

        accuracy = correct_chars / total_chars_in_ref
        
        return accuracy

    @staticmethod
    def calculate_bleu_score(predicted: str, reference: str) -> float:
        """Calculate BLEU score using sacrebleu."""
        pred_clean = EvaluationMetrics.clean_text(predicted)
        ref_clean = EvaluationMetrics.clean_text(reference)

        if not ref_clean or not pred_clean:
            return 0.0

        bleu = sacrebleu.corpus_bleu([pred_clean], [[ref_clean]], tokenize='zh')
        return bleu.score / 100.0

    @staticmethod
    def calculate_chrf_plus_plus(predicted: str, reference: str) -> float:
        """Calculate chrF++ score using sacrebleu."""
        pred_clean = EvaluationMetrics.clean_text(predicted)
        ref_clean = EvaluationMetrics.clean_text(reference)

        if not ref_clean or not pred_clean:
            return 0.0

        # Use standard chrF++ parameters (char_order=6, word_order=2, beta=2)
        chrf = sacrebleu.corpus_chrf([pred_clean], [[ref_clean]], word_order=2)
        return chrf.score / 100.0

    @staticmethod
    def calculate_all_metrics(predicted: str, reference: str) -> Dict[str, float]:
        """Calculate all four metrics."""
        return {
            'cer': EvaluationMetrics.calculate_cer(predicted, reference),
            'character_accuracy': EvaluationMetrics.calculate_character_accuracy(predicted, reference),
            'bleu_score': EvaluationMetrics.calculate_bleu_score(predicted, reference),
            'chrf_plus_plus': EvaluationMetrics.calculate_chrf_plus_plus(predicted, reference)
        }


class BaseEvaluator(ABC):
    """Abstract base class for the unified evaluation framework."""
    
    def __init__(self, evaluation_mode: str = "text_only", show_matched_text: bool = True):
        """
        Initialize the evaluator.

        Args:
            evaluation_mode: Evaluation mode
                - "segmented": segment-level matching
                - "text_only": full-text matching
            show_matched_text: Whether to print matched text in the terminal.
        """
        self.evaluation_mode = evaluation_mode
        self.show_matched_text = show_matched_text
        self.metrics_calculator = EvaluationMetrics()
        
        print(f"ðŸ”§ è¯„ä¼°æ¨¡å¼: {evaluation_mode}")
        if show_matched_text:
            print(f"ðŸ“ å°†æ˜¾ç¤ºåŒ¹é…çš„æ–‡æœ¬å†…å®¹")

    @abstractmethod
    def load_reference_data(self) -> Dict[str, Any]:
        """Load reference data (subtitle files, ground truth, etc.)."""
        pass

    @abstractmethod
    def load_prediction_data(self) -> List[Dict]:
        """Load prediction results data."""
        pass

    @abstractmethod
    def match_predictions_to_references(self) -> List[Dict]:
        """Match predictions to reference data."""
        pass

    @abstractmethod
    def get_full_text_for_item(self, item_id: str) -> Tuple[str, str]:
        """Get full predicted and reference text for a specific item."""
        pass

    def calculate_metrics_for_item_segmented(self, matches: List[Dict]) -> Dict:
        """Calculate metrics for a single item (segmented mode)."""
        if not matches:
            return {
                'total_segments': 0,
                'matched_segments': 0,
                'cer': 0.0,
                'character_accuracy': 0.0,
                'bleu_score': 0.0,
                'chrf_plus_plus': 0.0
            }

        # Calculate metrics for each match
        all_metrics = []
        for match in matches:
            predicted = match.get('prediction', '')
            reference = match.get('reference', '')
            metrics = self.metrics_calculator.calculate_all_metrics(predicted, reference)
            all_metrics.append(metrics)

        # Compute averages
        result = {
            'matched_segments': len(matches),
            'cer': sum(m['cer'] for m in all_metrics) / len(all_metrics),
            'character_accuracy': sum(m['character_accuracy'] for m in all_metrics) / len(all_metrics),
            'bleu_score': sum(m['bleu_score'] for m in all_metrics) / len(all_metrics),
            'chrf_plus_plus': sum(m['chrf_plus_plus'] for m in all_metrics) / len(all_metrics)
        }
        
        return result

    def calculate_metrics_for_item_text_only(self, item_id: str) -> Dict:
        """Calculate metrics for a single item (full-text mode)."""
        predicted_text, reference_text = self.get_full_text_for_item(item_id)

        if self.show_matched_text:
            print(f"\nðŸ“ é¡¹ç›® {item_id} çš„åŒ¹é…æ–‡æœ¬ (æ•´ä½“åˆå¹¶):")
            print(f"å‚è€ƒæ–‡æœ¬ ({len(reference_text)} å­—ç¬¦): {reference_text}")
            print(f"é¢„æµ‹æ–‡æœ¬ ({len(predicted_text)} å­—ç¬¦): {predicted_text}")
            print("-" * 60)

        metrics = self.metrics_calculator.calculate_all_metrics(predicted_text, reference_text)
        
        return {
            'item_id': item_id,
            'predicted_text_length': len(predicted_text),
            'reference_text_length': len(reference_text),
            'predicted_text': predicted_text,
            'reference_text': reference_text,
            **metrics
        }

    def calculate_overall_metrics(self, item_metrics: List[Dict]) -> Dict:
        """Calculate overall metrics."""
        if not item_metrics:
            return {
                'cer': 1.0,  # Worst CER
                'character_accuracy': 0.0,  # Worst accuracy
                'bleu_score': 0.0,  # Worst BLEU
                'chrf_plus_plus': 0.0,  # Worst chrF++
                'evaluation_mode': self.evaluation_mode,
                'total_items': 0
            }

        if self.evaluation_mode == "text_only":
            # For full-text mode, recompute after concatenation
            all_predicted = " ".join(item['predicted_text'] for item in item_metrics)
            all_reference = " ".join(item['reference_text'] for item in item_metrics)
            overall_metrics = self.metrics_calculator.calculate_all_metrics(all_predicted, all_reference)
        else:
            # For segmented mode, average the metrics
            overall_metrics = {
                'cer': sum(item['cer'] for item in item_metrics) / len(item_metrics),
                'character_accuracy': sum(item['character_accuracy'] for item in item_metrics) / len(item_metrics),
                'bleu_score': sum(item['bleu_score'] for item in item_metrics) / len(item_metrics),
                'chrf_plus_plus': sum(item['chrf_plus_plus'] for item in item_metrics) / len(item_metrics)
            }

        overall_metrics.update({
            'evaluation_mode': self.evaluation_mode,
            'total_items': len(item_metrics)
        })

        return overall_metrics

    def display_results(self, overall_metrics: Dict, item_metrics: List[Dict]):
        """Display evaluation results."""
        mode_name = "æ•´ä½“æ–‡æœ¬åŒ¹é…æ¨¡å¼" if self.evaluation_mode == "text_only" else "é€å¥åŒ¹é…æ¨¡å¼"
        print(f"\nðŸ“ˆ è¯„ä¼°ç»“æžœ ({mode_name}):")
        print("=" * 80)

        print(f"\nðŸŽ¯ æ€»ä½“æŒ‡æ ‡:")
        print(f"  CER (å­—ç¬¦é”™è¯¯çŽ‡): {overall_metrics['cer']:.4f}")
        print(f"  å­—ç¬¦å‡†ç¡®çŽ‡: {overall_metrics['character_accuracy']:.4f} ({overall_metrics['character_accuracy']*100:.2f}%)")
        print(f"  BLEUåˆ†æ•°: {overall_metrics['bleu_score']:.4f}")
        print(f"  chrF++åˆ†æ•°: {overall_metrics['chrf_plus_plus']:.4f}")

        print(f"\nðŸ’¡ è¯„ä¼°è¯´æ˜Ž:")
        print(f"  - ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°æ¡†æž¶ (BaseEvaluator)")
        print(f"  - æ ‡å‡†åº“: jiwer (CER) + sacrebleu (BLEU, chrF++)")
        print(f"  - è¯„ä¼°æ¨¡å¼: {mode_name}")
        print(f"  - å¼‚å¸¸è¾“å‡ºå·²æ ‡å‡†åŒ–å¤„ç†")

        print(f"\nðŸ“‹ å„é¡¹ç›®è¯¦ç»†æŒ‡æ ‡:")
        if item_metrics:
            sorted_items = sorted(item_metrics, key=lambda x: x.get('character_accuracy', 0), reverse=True)
            
            for item in sorted_items[:10]:  # æ˜¾ç¤ºå‰10ä¸ªé¡¹ç›®
                item_id = item.get('item_id', 'Unknown')
                print(f"  {item_id}: CER={item['cer']:.3f}, ACC={item['character_accuracy']:.3f}, "
                      f"BLEU={item['bleu_score']:.3f}, chrF++={item['chrf_plus_plus']:.3f}")

    def save_results(self, overall_metrics: Dict, item_metrics: List[Dict], output_path: str):
        """Save evaluation results."""
        output_data = {
            'evaluation_mode': self.evaluation_mode,
            'overall_metrics': overall_metrics,
            'item_metrics': item_metrics,
            'framework_version': 'unified_evaluation_framework_v1.0'
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ’¾ è¯„ä¼°ç»“æžœå·²ä¿å­˜åˆ°: {output_path}")

    def run_evaluation(self) -> Tuple[Dict, List[Dict]]:
        """Run the full evaluation pipeline."""
        print(f"\nðŸš€ å¼€å§‹è¿è¡Œç»Ÿä¸€è¯„ä¼°æ¡†æž¶...")
        
        # Load data
        self.load_reference_data()
        self.load_prediction_data()
        
        # Execute logic based on evaluation mode
        if self.evaluation_mode == "segmented":
            matches = self.match_predictions_to_references()
            item_metrics = self.calculate_segmented_evaluation(matches)
        else:  # text_only
            item_metrics = self.calculate_text_only_evaluation()
        
        # Compute overall metrics
        overall_metrics = self.calculate_overall_metrics(item_metrics)
        
        # Display results
        self.display_results(overall_metrics, item_metrics)
        
        return overall_metrics, item_metrics

    @abstractmethod
    def calculate_segmented_evaluation(self, matches: List[Dict]) -> List[Dict]:
        """Calculate segmented evaluation results."""
        pass

    @abstractmethod
    def calculate_text_only_evaluation(self) -> List[Dict]:
        """Calculate full-text evaluation results."""
        pass
