import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def test_translation():
    base_model_id = "Qwen/Qwen3-VL-4B-Instruct" # Wait, Qwen3-VL 4B is a VL model.
    # For translation, we use the language model part.
    adapter_path = "translation/qwen3vl_translation_lora_v2_output/fold_0/final_model"
    
    print(f"ğŸš€ Loading base model: {base_model_id}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
    from transformers import Qwen3VLForConditionalGeneration
    model = Qwen3VLForConditionalGeneration.from_pretrained(
        base_model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"ğŸ“¦ Loading adapter: {adapter_path}")
    model = PeftModel.from_pretrained(model, adapter_path)
    model.eval()
    
    test_queries = [
        "Translate Chinese to Japanese: å®‹æ°é›†å›¢çš„é¢è¯•å¯¹æˆ‘æ¥è¯´éå¸¸é‡è¦ã€‚",
        "Translate Chinese to Japanese: æˆ‘ä¸€å®šä¼šæŸ¥æ¸…æ¥šæ¯äº²å»ä¸–çš„çœŸç›¸ã€‚",
        "Translate Chinese to Japanese: æ—¢ç„¶ä½ è¿™ä¹ˆæƒ³å«è¿›å®‹å®¶ï¼Œé‚£æˆ‘å°±æˆå…¨ä½ ã€‚"
    ]
    
    for query in test_queries:
        messages = [{"role": "user", "content": query}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=128,
                do_sample=False
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(f"\nQ: {query}")
            print(f"A: {response}")

if __name__ == "__main__":
    test_translation()
