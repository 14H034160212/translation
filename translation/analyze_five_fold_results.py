#!/usr/bin/env python3
"""Analyze 5-fold cross-validation results."""

import json
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

def load_fold_results(output_dir: str):
    output_path = Path(output_dir)
    results = {}
    
    for fold in range(5):
        fold_dir = output_path / f"fold_{fold}"
        if not fold_dir.exists():
            print(f"âš ï¸ Fold {fold + 1} directory not found: {fold_dir}")
            continue
            
        config_path = fold_dir / "training_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                results[f"fold_{fold}"] = {
                    "config": json.load(f)
                }
        
        splits_path = fold_dir / "data_splits.json"
        if splits_path.exists():
            with open(splits_path, 'r', encoding='utf-8') as f:
                results[f"fold_{fold}"]["splits"] = json.load(f)
        
        history_path = fold_dir / "training_history.json"
        if history_path.exists():
            with open(history_path, 'r', encoding='utf-8') as f:
                results[f"fold_{fold}"]["history"] = json.load(f)
    
    return results

def analyze_training_history(results):
    print("ğŸ“Š Analyzing training history...")
    
    all_metrics = []
    for fold_name, fold_data in results.items():
        if "history" in fold_data:
            history = fold_data["history"]
            for step_data in history:
                if "eval_loss" in step_data:
                    all_metrics.append({
                        "fold": fold_name,
                        "step": step_data.get("step", 0),
                        "eval_loss": step_data["eval_loss"],
                        "train_loss": step_data.get("loss", None)
                    })
    
    if all_metrics:
        df = pd.DataFrame(all_metrics)
        
        best_losses = df.groupby("fold")["eval_loss"].min()
        print("\nğŸ† Best validation loss per fold:")
        for fold, loss in best_losses.items():
            print(f"  {fold}: {loss:.4f}")
        
        print(f"\nğŸ“ˆ Overall statistics:")
        print(f"  Average best validation loss: {best_losses.mean():.4f}")
        print(f"  Std dev of best validation loss: {best_losses.std():.4f}")
        print(f"  Range: {best_losses.min():.4f} - {best_losses.max():.4f}")
        
        return df, best_losses
    else:
        print("âŒ No training history data found")
        return None, None

def analyze_data_splits(results):
    """Analyze data splits."""
    print("\nğŸ“‹ åˆ†ææ•°æ®åˆ†å‰²...")
    
    splits_info = []
    for fold_name, fold_data in results.items():
        if "splits" in fold_data:
            splits = fold_data["splits"]
            splits_info.append({
                "fold": fold_name,
                "total_samples": splits["total_samples"],
                "train_samples": splits["train_samples"],
                "val_samples": splits["val_samples"],
                "train_ratio": splits["train_samples"] / splits["total_samples"],
                "val_ratio": splits["val_samples"] / splits["total_samples"]
            })
    
    if splits_info:
        df = pd.DataFrame(splits_info)
        print("\nğŸ“Š æ•°æ®åˆ†å‰²ç»Ÿè®¡:")
        print(df.to_string(index=False))
        
        # Check data split consistency
        total_samples = df["total_samples"].iloc[0]
        train_samples_mean = df["train_samples"].mean()
        val_samples_mean = df["val_samples"].mean()
        
        print(f"\nğŸ” æ•°æ®åˆ†å‰²ä¸€è‡´æ€§æ£€æŸ¥:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  å¹³å‡è®­ç»ƒæ ·æœ¬æ•°: {train_samples_mean:.1f}")
        print(f"  å¹³å‡éªŒè¯æ ·æœ¬æ•°: {val_samples_mean:.1f}")
        print(f"  è®­ç»ƒé›†æ¯”ä¾‹: {train_samples_mean/total_samples:.3f}")
        print(f"  éªŒè¯é›†æ¯”ä¾‹: {val_samples_mean/total_samples:.3f}")
        
        return df
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®åˆ†å‰²ä¿¡æ¯")
        return None

def create_visualizations(results, output_dir: str):
    """Create visualizations."""
    print("\nğŸ¨ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
    
    output_path = Path(output_dir)
    
    # Load training history data
    all_metrics = []
    for fold_name, fold_data in results.items():
        if "history" in fold_data:
            history = fold_data["history"]
            for step_data in history:
                if "eval_loss" in step_data:
                    all_metrics.append({
                        "fold": fold_name,
                        "step": step_data.get("step", 0),
                        "eval_loss": step_data["eval_loss"],
                        "train_loss": step_data.get("loss", None)
                    })
    
    if not all_metrics:
        print("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®åˆ›å»ºå¯è§†åŒ–")
        return
    
    df = pd.DataFrame(all_metrics)
    
    # Create charts
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle("äº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒç»“æœåˆ†æ", fontsize=16)
    
    # 1. Validation loss curve
    ax1 = axes[0, 0]
    for fold in df["fold"].unique():
        fold_data = df[df["fold"] == fold]
        ax1.plot(fold_data["step"], fold_data["eval_loss"], label=fold, marker='o', markersize=3)
    ax1.set_xlabel("è®­ç»ƒæ­¥æ•°")
    ax1.set_ylabel("éªŒè¯æŸå¤±")
    ax1.set_title("éªŒè¯æŸå¤±æ›²çº¿")
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Training loss curve
    ax2 = axes[0, 1]
    for fold in df["fold"].unique():
        fold_data = df[df["fold"] == fold]
        if "train_loss" in fold_data.columns and fold_data["train_loss"].notna().any():
            ax2.plot(fold_data["step"], fold_data["train_loss"], label=fold, marker='s', markersize=3)
    ax2.set_xlabel("è®­ç»ƒæ­¥æ•°")
    ax2.set_ylabel("è®­ç»ƒæŸå¤±")
    ax2.set_title("è®­ç»ƒæŸå¤±æ›²çº¿")
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Best validation loss per fold
    ax3 = axes[1, 0]
    best_losses = df.groupby("fold")["eval_loss"].min()
    folds = list(best_losses.index)
    losses = list(best_losses.values)
    bars = ax3.bar(folds, losses, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])
    ax3.set_xlabel("æŠ˜æ•°")
    ax3.set_ylabel("æœ€ä½³éªŒè¯æŸå¤±")
    ax3.set_title("æ¯ä¸ªæŠ˜çš„æœ€ä½³éªŒè¯æŸå¤±")
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Add value labels
    for bar, loss in zip(bars, losses):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{loss:.4f}', ha='center', va='bottom')
    
    # 4. Loss distribution box plot
    ax4 = axes[1, 1]
    loss_data = [df[df["fold"] == fold]["eval_loss"].values for fold in folds]
    box_plot = ax4.boxplot(loss_data, labels=folds, patch_artist=True)
    ax4.set_xlabel("æŠ˜æ•°")
    ax4.set_ylabel("éªŒè¯æŸå¤±")
    ax4.set_title("éªŒè¯æŸå¤±åˆ†å¸ƒ")
    ax4.grid(True, alpha=0.3, axis='y')
    
    # Set colors
    colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
    
    plt.tight_layout()
    
    # Save plot
    plot_path = output_path / "five_fold_analysis.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.show()

def generate_summary_report(results, output_dir: str):
    """Generate the summary report."""
    print("\nğŸ“ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    
    output_path = Path(output_dir)
    report_path = output_path / "five_fold_summary_report.txt"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("äº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒç»“æœæ±‡æ€»æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")
        
        # Basic info
        f.write("ğŸ“‹ åŸºæœ¬ä¿¡æ¯:\n")
        f.write(f"  è¾“å‡ºç›®å½•: {output_dir}\n")
        f.write(f"  æ€»æŠ˜æ•°: 5\n")
        f.write(f"  å®ŒæˆæŠ˜æ•°: {len(results)}\n\n")
        
        # Per-fold configuration
        f.write("âš™ï¸ è®­ç»ƒé…ç½®:\n")
        for fold_name, fold_data in results.items():
            if "config" in fold_data:
                config = fold_data["config"]
                f.write(f"  {fold_name}:\n")
                f.write(f"    æ¨¡å‹: {config.get('model_name', 'N/A')}\n")
                f.write(f"    å­¦ä¹ ç‡: {config.get('learning_rate', 'N/A')}\n")
                f.write(f"    æ‰¹æ¬¡å¤§å°: {config.get('batch_size', 'N/A')}\n")
                f.write(f"    è®­ç»ƒè½®æ•°: {config.get('num_epochs', 'N/A')}\n")
                f.write(f"    LoRA r: {config.get('lora_r', 'N/A')}\n")
                f.write(f"    LoRA alpha: {config.get('lora_alpha', 'N/A')}\n\n")
        
        # Data split info
        f.write("ğŸ“Š æ•°æ®åˆ†å‰²ä¿¡æ¯:\n")
        for fold_name, fold_data in results.items():
            if "splits" in fold_data:
                splits = fold_data["splits"]
                f.write(f"  {fold_name}:\n")
                f.write(f"    æ€»æ ·æœ¬æ•°: {splits.get('total_samples', 'N/A')}\n")
                f.write(f"    è®­ç»ƒæ ·æœ¬æ•°: {splits.get('train_samples', 'N/A')}\n")
                f.write(f"    éªŒè¯æ ·æœ¬æ•°: {splits.get('val_samples', 'N/A')}\n\n")
        
        # Training results
        f.write("ğŸ† è®­ç»ƒç»“æœ:\n")
        all_metrics = []
        for fold_name, fold_data in results.items():
            if "history" in fold_data:
                history = fold_data["history"]
                eval_losses = [step.get("eval_loss") for step in history if "eval_loss" in step]
                if eval_losses:
                    best_loss = min(eval_losses)
                    final_loss = eval_losses[-1]
                    all_metrics.append((fold_name, best_loss, final_loss))
                    f.write(f"  {fold_name}:\n")
                    f.write(f"    æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}\n")
                    f.write(f"    æœ€ç»ˆéªŒè¯æŸå¤±: {final_loss:.6f}\n\n")
        
        # Overall statistics
        if all_metrics:
            best_losses = [metrics[1] for metrics in all_metrics]
            final_losses = [metrics[2] for metrics in all_metrics]
            
            f.write("ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡æœ€ä½³éªŒè¯æŸå¤±: {np.mean(best_losses):.6f}\n")
            f.write(f"  æœ€ä½³éªŒè¯æŸå¤±æ ‡å‡†å·®: {np.std(best_losses):.6f}\n")
            f.write(f"  æœ€ä½³éªŒè¯æŸå¤±èŒƒå›´: {min(best_losses):.6f} - {max(best_losses):.6f}\n")
            f.write(f"  å¹³å‡æœ€ç»ˆéªŒè¯æŸå¤±: {np.mean(final_losses):.6f}\n")
            f.write(f"  æœ€ç»ˆéªŒè¯æŸå¤±æ ‡å‡†å·®: {np.std(final_losses):.6f}\n\n")
        
        f.write("=" * 50 + "\n")
        f.write("æŠ¥å‘Šç”Ÿæˆå®Œæˆ\n")
    
    print(f"ğŸ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def main():
    """Main entry point."""
    output_dir = "translation/chinese_japanese_lora_output"
    
    print("ğŸ” å¼€å§‹åˆ†æäº”æŠ˜äº¤å‰éªŒè¯ç»“æœ...")
    
    # Load results
    results = load_fold_results(output_dir)
    
    if not results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®­ç»ƒç»“æœ")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(results)} æŠ˜çš„è®­ç»ƒç»“æœ")
    
    # Analyze training history
    history_df, best_losses = analyze_training_history(results)
    
    # Analyze data splits
    splits_df = analyze_data_splits(results)
    
    # Create visualizations
    create_visualizations(results, output_dir)
    
    # Generate summary report
    generate_summary_report(results, output_dir)
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 
