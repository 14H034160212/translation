#!/usr/bin/env python3
"""
Five-Fold Cross-Validation Model Evaluation Script
Evaluate all five-fold cross-validation models and generate a comprehensive report.
Use serial model loading to avoid running out of GPU memory.
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import json
import time
import csv
from datetime import datetime
import torch
import re
import numpy as np
import pandas as pd
import gc

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from unified_translator_evaluator import TranslationEvaluator

class FiveFoldModelEvaluator:
    def __init__(self, 
                 base_model_name: str = "Qwen/Qwen2.5-3B-Instruct",
                 output_dir: str = "translation/chinese_japanese_lora_output",
                 test_data_path: str = None):
        
        self.base_model_name = base_model_name
        self.output_dir = Path(output_dir)
        
        # If no test data path is provided, reconstruct from split info
        if test_data_path:
            self.test_data_path = Path(test_data_path)
        else:
            self.test_data_path = None
            
        # Helper to strip speaker identifiers
        self.clean_speaker_identifier = lambda text: re.sub(r'^\d+:\s*', '', text.strip())
        
        # Store evaluation results only (no models)
        self.fold_results = {}
        
    def load_base_model(self):
        """Load the base model for comparison."""
        print("ğŸ”§ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
        
        self.base_tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        if self.base_tokenizer.pad_token is None:
            self.base_tokenizer.pad_token = self.base_tokenizer.eos_token
            
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
        
    def unload_base_model(self):
        """Unload the base model to free GPU memory."""
        print("ğŸ—‘ï¸ æ­£åœ¨å¸è½½åŸºç¡€æ¨¡å‹...")
        if hasattr(self, 'base_model'):
            del self.base_model
        if hasattr(self, 'base_tokenizer'):
            del self.base_tokenizer
        
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        print("âœ… åŸºç¡€æ¨¡å‹å·²å¸è½½")
        
    def load_single_fold_model(self, fold: int) -> Tuple[PeftModel, AutoTokenizer]:
        """Load a single fold model."""
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½ç¬¬ {fold + 1} æŠ˜æ¨¡å‹...")
        
        fold_dir = self.output_dir / f"fold_{fold}"
        if not fold_dir.exists():
            raise FileNotFoundError(f"ç¬¬ {fold + 1} æŠ˜ç›®å½•ä¸å­˜åœ¨: {fold_dir}")
            
        model_path = fold_dir / "final_model"
        if not model_path.exists():
            raise FileNotFoundError(f"ç¬¬ {fold + 1} æŠ˜æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            
        try:
            # Load base model
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Load LoRA weights
            fold_model = PeftModel.from_pretrained(base_model, str(model_path))
            
            # Load tokenizer
            fold_tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True,
                padding_side="right"
            )
            
            if fold_tokenizer.pad_token is None:
                fold_tokenizer.pad_token = fold_tokenizer.eos_token
            
            print(f"âœ… ç¬¬ {fold + 1} æŠ˜æ¨¡å‹åŠ è½½å®Œæˆ")
            return fold_model, fold_tokenizer
            
        except Exception as e:
            print(f"âŒ ç¬¬ {fold + 1} æŠ˜æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
            
    def unload_fold_model(self, model: PeftModel, tokenizer: AutoTokenizer):
        """Unload a fold model to free GPU memory."""
        print("ğŸ—‘ï¸ æ­£åœ¨å¸è½½æŠ˜æ¨¡å‹...")
        del model
        del tokenizer
        
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        print("âœ… æŠ˜æ¨¡å‹å·²å¸è½½")
        
    def load_test_data(self) -> List[Dict]:
        """Load test data."""
        print("ğŸ“Š æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...")
        
        # If no test data path is provided, reconstruct from split info
        if self.test_data_path is None or not self.test_data_path.exists():
            print("ğŸ” å°è¯•ä»æ•°æ®åˆ†å‰²ä¿¡æ¯é‡å»ºæµ‹è¯•æ•°æ®...")
            test_data = self.reconstruct_test_data()
        else:
            with open(self.test_data_path, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
                
        print(f"âœ… åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_data
        
    def reconstruct_test_data(self) -> List[Dict]:
        """Reconstruct test data from split info."""
        print("ğŸ”§ ä»æ•°æ®åˆ†å‰²ä¿¡æ¯é‡å»ºæµ‹è¯•æ•°æ®...")
        
        # Load the first fold's split info to get the full dataset
        first_fold_dir = self.output_dir / "fold_0"
        if not first_fold_dir.exists():
            raise FileNotFoundError(f"ç¬¬ä¸€ä¸ªæŠ˜ç›®å½•ä¸å­˜åœ¨: {first_fold_dir}")
            
        splits_path = first_fold_dir / "data_splits.json"
        if not splits_path.exists():
            raise FileNotFoundError(f"æ•°æ®åˆ†å‰²ä¿¡æ¯ä¸å­˜åœ¨: {splits_path}")
            
        with open(splits_path, 'r', encoding='utf-8') as f:
            splits_info = json.load(f)
            
        # Load raw data from the data directory
        data_dir = Path("data")
        chinese_dir = data_dir / "Chinese"
        japanese_dir = data_dir / "Japanese"
        
        if not chinese_dir.exists() or not japanese_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {chinese_dir} æˆ– {japanese_dir}")
            
        # Read all files
        chinese_files = {}
        japanese_files = {}
        
        for file_path in sorted(chinese_dir.glob("*.txt")):
            with open(file_path, 'r', encoding='utf-8') as f:
                chinese_files[file_path.stem] = f.read().strip()
                
        for file_path in sorted(japanese_dir.glob("*.txt")):
            with open(file_path, 'r', encoding='utf-8') as f:
                japanese_files[file_path.stem] = f.read().strip()
        
        # Rebuild test data (use the first fold's validation set)
        test_data = []
        val_ids = splits_info.get("val_ids", [])
        
        for file_id in val_ids:
            if file_id in chinese_files and file_id in japanese_files:
                chinese_text = chinese_files[file_id]
                japanese_text = japanese_files[file_id]
                
                # Strip speaker identifiers
                chinese_clean = self.clean_speaker_identifier(chinese_text)
                japanese_clean = self.clean_speaker_identifier(japanese_text)
                
                if chinese_clean and japanese_clean:
                    test_data.append({
                        "id": file_id,
                        "chinese": chinese_clean,
                        "japanese": japanese_clean
                    })
        
        print(f"âœ… é‡å»ºäº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_data
        
    def translate_with_model(self, text: str, model, tokenizer, model_name: str) -> str:
        """Translate using the specified model."""
        try:
            # Strip speaker identifiers
            cleaned_text = self.clean_speaker_identifier(text)
            
            # Build messages in the exact format used by the original script
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘å‘˜ã€‚è¯·å°†ä¸‹é¢çš„ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆæ—¥æ–‡ã€‚åªè¾“å‡ºç¿»è¯‘åçš„æ—¥æ–‡æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹ã€‚"},
                {"role": "user", "content": cleaned_text}
            ]
            
            # Use the same parameters as the original script
            max_tokens = 2048
            temperature = 0.0
            
            # Build inputs
            input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            
            # Generate translation
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # Decode output
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract translation (remove input portion)
            translation = response.replace(input_text, "").strip()
            
            return translation
            
        except Exception as e:
            print(f"    âŒ {model_name} ç¿»è¯‘å¤±è´¥: {e}")
            return f"FAILED_{model_name}"
            
    def evaluate_base_model(self, test_data: List[Dict], evaluator: TranslationEvaluator) -> Dict:
        """Evaluate the base model."""
        print("\nğŸ” è¯„ä¼°åŸºç¡€æ¨¡å‹...")
        
        base_hypotheses = []
        base_references = []
        base_comet_scores = []
        
        for i, item in enumerate(test_data):
            print(f"  ğŸ“ å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)} (ID: {item['id']})")
            
            chinese_text = item["chinese"]
            japanese_reference = item["japanese"]
            
            translation = self.translate_with_model(
                chinese_text, self.base_model, self.base_tokenizer, "åŸºç¡€æ¨¡å‹"
            )
            
            base_hypotheses.append(translation)
            base_references.append(japanese_reference)
            
            # Compute COMET score
            if evaluator.comet_model and "FAILED" not in translation:
                try:
                    comet_data = [{"src": chinese_text, "mt": translation, "ref": japanese_reference}]
                    score = evaluator.comet_model.predict(comet_data, batch_size=1, gpus=0).scores[0]
                    base_comet_scores.append(score)
                except Exception as e:
                    print(f"    âŒ COMET è®¡ç®—å¤±è´¥: {e}")
        
        # Compute base model metrics
        base_metrics = evaluator.calculate_bleu_chrf_ter(base_hypotheses, base_references)
        if base_comet_scores:
            base_metrics["COMET (Average)"] = sum(base_comet_scores) / len(base_comet_scores)
        else:
            base_metrics["COMET (Average)"] = 0.0
            
        # Save base model results
        self.base_results = {
            "metrics": base_metrics,
            "hypotheses": base_hypotheses,
            "references": base_references,
            "comet_scores": base_comet_scores,
            "test_samples": len(test_data)
        }
        
        print("âœ… åŸºç¡€æ¨¡å‹è¯„ä¼°å®Œæˆ")
        return base_metrics
        
    def evaluate_single_fold(self, fold: int, test_data: List[Dict], evaluator: TranslationEvaluator) -> Dict:
        """Evaluate a single fold model."""
        print(f"\nğŸ” å¼€å§‹è¯„ä¼°ç¬¬ {fold + 1} æŠ˜...")
        
        # Load model
        model, tokenizer = self.load_single_fold_model(fold)
        
        try:
            hypotheses = []
            references = []
            sources = []
            comet_scores = []
            
            # Evaluate each test sample
            for i, item in enumerate(test_data):
                print(f"  ğŸ“ å¤„ç†æ ·æœ¬ {i+1}/{len(test_data)} (ID: {item['id']})")
                
                chinese_text = item["chinese"]
                japanese_reference = item["japanese"]
                
                # Translate
                translation = self.translate_with_model(
                    chinese_text, model, tokenizer, f"ç¬¬{fold + 1}æŠ˜"
                )
                
                hypotheses.append(translation)
                references.append(japanese_reference)
                sources.append(chinese_text)
                
                # Compute COMET score
                if evaluator.comet_model and "FAILED" not in translation:
                    try:
                        comet_data = [{"src": chinese_text, "mt": translation, "ref": japanese_reference}]
                        score = evaluator.comet_model.predict(comet_data, batch_size=1, gpus=0).scores[0]
                        comet_scores.append(score)
                    except Exception as e:
                        print(f"    âŒ COMET è®¡ç®—å¤±è´¥: {e}")
            
            # Compute evaluation metrics
            metrics = evaluator.calculate_bleu_chrf_ter(hypotheses, references)
            if comet_scores:
                metrics["COMET (Average)"] = sum(comet_scores) / len(comet_scores)
            else:
                metrics["COMET (Average)"] = 0.0
                
            # Save detailed results
            fold_results = {
                "metrics": metrics,
                "hypotheses": hypotheses,
                "references": references,
                "sources": sources,
                "comet_scores": comet_scores,
                "test_samples": len(test_data)
            }
            
            self.fold_results[f"fold_{fold}"] = fold_results
            
            print(f"âœ… ç¬¬ {fold + 1} æŠ˜è¯„ä¼°å®Œæˆ")
            return metrics
            
        finally:
            # Ensure model is unloaded
            self.unload_fold_model(model, tokenizer)
        
    def evaluate_all_models(self, output_dir: str = "translation/five_fold_evaluation_results"):
        """Evaluate all models in serial."""
        print("ğŸš€ å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯æ¨¡å‹è¯„ä¼°ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
        print("=" * 80)
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Load test data
        test_data = self.load_test_data()
        
        # Initialize evaluator
        evaluator = TranslationEvaluator()
        
        # Load base model
        self.load_base_model()
        
        # Evaluate base model
        base_metrics = self.evaluate_base_model(test_data, evaluator)
        
        # Unload base model to free GPU memory
        self.unload_base_model()
        
        # Evaluate each fold serially
        print(f"\nğŸ” ä¸²è¡Œè¯„ä¼°æ‰€æœ‰æŠ˜çš„æ¨¡å‹...")
        fold_metrics = {}
        
        for fold in range(5):
            try:
                metrics = self.evaluate_single_fold(fold, test_data, evaluator)
                fold_metrics[f"fold_{fold}"] = metrics
            except Exception as e:
                print(f"âŒ ç¬¬ {fold + 1} æŠ˜è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # Generate evaluation report
        self.generate_evaluation_report(output_path, base_metrics, fold_metrics, test_data)
        
        print("\nğŸ‰ äº”æŠ˜äº¤å‰éªŒè¯è¯„ä¼°å®Œæˆï¼")
        
    def generate_evaluation_report(self, output_path: Path, base_metrics: Dict, fold_metrics: Dict, test_data: List[Dict]):
        """Generate the evaluation report."""
        print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Generate detailed CSV results
        csv_file_path = output_path / f"five_fold_evaluation_{timestamp}.csv"
        self.generate_detailed_csv(csv_file_path, test_data)
        
        # 2. Generate metric comparison report
        summary_file = output_path / f"five_fold_evaluation_summary_{timestamp}.json"
        self.generate_summary_json(summary_file, base_metrics, fold_metrics)
        
        # 3. Print comparison results
        self.print_comparison_results(base_metrics, fold_metrics)
        
        # 4. Generate statistical analysis
        stats_file = output_path / f"five_fold_statistics_{timestamp}.txt"
        self.generate_statistics_report(stats_file, base_metrics, fold_metrics)
        
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {csv_file_path}")
        print(f"âœ… å¯¹æ¯”æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        print(f"âœ… ç»Ÿè®¡åˆ†æå·²ä¿å­˜åˆ°: {stats_file}")
        
    def generate_detailed_csv(self, csv_file_path: Path, test_data: List[Dict]):
        """Generate the detailed CSV results file."""
        csv_header = ["id", "chinese_text", "japanese_reference", "base_translation"]
        
        # Add columns for each fold
        for fold_name in self.fold_results.keys():
            csv_header.extend([f"{fold_name}_translation", f"{fold_name}_comet"])
            
        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_header)
            writer.writeheader()
            
            for i, item in enumerate(test_data):
                row = {
                    "id": item["id"],
                    "chinese_text": item["chinese"],
                    "japanese_reference": item["japanese"]
                }
                
                # Add base model results
                if hasattr(self, 'base_results'):
                    row["base_translation"] = self.base_results["hypotheses"][i]
                
                # Add results for each fold
                for fold_name in self.fold_results.keys():
                    if fold_name in self.fold_results:
                        fold_data = self.fold_results[fold_name]
                        row[f"{fold_name}_translation"] = fold_data["hypotheses"][i]
                        row[f"{fold_name}_comet"] = fold_data["comet_scores"][i] if i < len(fold_data["comet_scores"]) else "N/A"
                
                writer.writerow(row)
                
    def generate_summary_json(self, summary_file: Path, base_metrics: Dict, fold_metrics: Dict):
        """Generate the JSON summary report."""
        summary = {
            "base_model": base_metrics,
            "fold_models": fold_metrics,
            "evaluation_info": {
                "total_folds": len(fold_metrics),
                "timestamp": datetime.now().isoformat()
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=4, ensure_ascii=False)
            
    def print_comparison_results(self, base_metrics: Dict, fold_metrics: Dict):
        """Print comparison results."""
        print("\n" + "=" * 100)
        print("ğŸ¯ äº”æŠ˜äº¤å‰éªŒè¯æ¨¡å‹è¯„ä¼°ç»“æœå¯¹æ¯”")
        print("=" * 100)
        
        # Table header
        header = f"{'æŒ‡æ ‡':<15} {'åŸºç¡€æ¨¡å‹':<15}"
        for fold_name in fold_metrics.keys():
            header += f" {fold_name:<15}"
        header += f" {'å¹³å‡':<15} {'æ ‡å‡†å·®':<15}"
        print(header)
        print("-" * 100)
        
        # Compute stats for each metric
        metrics_list = ["BLEU", "chrF++", "TER", "COMET (Average)"]
        
        for metric in metrics_list:
            if metric in base_metrics:
                base_score = base_metrics[metric]
                row = f"{metric:<15} {base_score:<15.4f}"
                
                fold_scores = []
                for fold_name in fold_metrics.keys():
                    if metric in fold_metrics[fold_name]:
                        score = fold_metrics[fold_name][metric]
                        fold_scores.append(score)
                        row += f" {score:<15.4f}"
                    else:
                        row += f" {'N/A':<15}"
                
                # Compute statistics
                if fold_scores:
                    mean_score = np.mean(fold_scores)
                    std_score = np.std(fold_scores)
                    row += f" {mean_score:<15.4f} {std_score:<15.4f}"
                else:
                    row += f" {'N/A':<15} {'N/A':<15}"
                
                print(row)
                
        print("-" * 100)
        
    def generate_statistics_report(self, stats_file: Path, base_metrics: Dict, fold_metrics: Dict):
        """Generate the statistical analysis report."""
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("äº”æŠ˜äº¤å‰éªŒè¯æ¨¡å‹è¯„ä¼°ç»Ÿè®¡åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("ğŸ“Š åŸºç¡€æ¨¡å‹æ€§èƒ½:\n")
            for metric, value in base_metrics.items():
                f.write(f"  {metric}: {value:.6f}\n")
            f.write("\n")
            
            f.write("ğŸ“ˆ å„æŠ˜æ¨¡å‹æ€§èƒ½:\n")
            for fold_name, metrics in fold_metrics.items():
                f.write(f"  {fold_name}:\n")
                for metric, value in metrics.items():
                    f.write(f"    {metric}: {value:.6f}\n")
                f.write("\n")
            
            # Compute improvement stats
            f.write("ğŸ“Š æ”¹è¿›ç»Ÿè®¡:\n")
            metrics_list = ["BLEU", "chrF++", "TER", "COMET (Average)"]
            
            for metric in metrics_list:
                if metric in base_metrics:
                    base_score = base_metrics[metric]
                    fold_scores = []
                    
                    for fold_name in fold_metrics.keys():
                        if metric in fold_metrics[fold_name]:
                            fold_scores.append(fold_metrics[fold_name][metric])
                    
                    if fold_scores:
                        improvements = [fold_score - base_score for fold_score in fold_scores]
                        mean_improvement = np.mean(improvements)
                        std_improvement = np.std(improvements)
                        
                        f.write(f"  {metric}:\n")
                        f.write(f"    å¹³å‡æ”¹è¿›: {mean_improvement:+.6f}\n")
                        f.write(f"    æ”¹è¿›æ ‡å‡†å·®: {std_improvement:.6f}\n")
                        f.write(f"    æ”¹è¿›èŒƒå›´: {min(improvements):+.6f} ~ {max(improvements):+.6f}\n")
                        f.write(f"    æ”¹è¿›æ ·æœ¬æ•°: {len(improvements)}/{len(fold_metrics)}\n\n")

def main():
    """Main entry point."""
    print("ğŸš€ å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯æ¨¡å‹è¯„ä¼°ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
    print("=" * 60)
    
    # Create evaluator
    evaluator = FiveFoldModelEvaluator()
    
    # Run evaluation (handles model loading/unloading)
    evaluator.evaluate_all_models()

if __name__ == "__main__":
    main() 
